View(bodies)
hept_noscore_mat = as.matrix(select(hept_df, -score))
# Question 1: Principal Component Analysis
library("psych")
library("dplyr")
library("HSAUR")
data(heptathlon)
# 1.1
# plot(heptathlon)
# intentionally commented, so I don't see it every I run my code.
# There are 56 plots on the 8 variables, and 25 observations in the data set.
# There are some pairwise relationships between variables that appear roughly
# linear, like run200 vs. score and longjump vs. hurdles. The highjump plots
# appear to have a significant outlier. Some, like javelin vs. shot, appear
# to have little recognizable correlations.
# 1.2
hept_df = mutate(heptathlon,
hurdles_t = max(hurdles) - hurdles,
run200m_t = max(run200m) - run200m,
run800m_t = max(run800m) - run800m
) %>%
select(., - hurdles, -run200m, -run800m)
# 1.3
# plot(heptathlon)
# intentionally commented, so I don't see it every I run my code.
# I don't see a difference here, and I wouldn't expect to. The transformations
# I performed above are translations only: I don't see how they would effect
# the relationships of any of these variables.
# 1.4
# first, I'll create the covariance matrix:
hept_noscore_mat = as.matrix(select(hept_df, -score))
View(hept_noscore_mat)
hept_noscore_cov = cov(hept_noscore_mat)
View(hept_noscore_cov)
data("longley")
View(longley)
dim(longley)
cor(longley)
hept_noscore_cor = cor(hept_noscore_mat)
source('~/Desktop/homework/CurseDimensionality/adamcone.R')
View(hept_noscore_cor)
fa.parallel(hept_noscore_cor,
n.obs = 25,
fa = "pc",
n.iter = 100)
abline(h = 1) #Adding a horizontal line at 1.
source('~/Desktop/homework/CurseDimensionality/adamcone.R')
pc_hept_noscore = principal(hept_noscore_cor,
nfactors = 2,
rotate = "none")
pc_hept_noscore
pc_hept_noscore
factor.plot(pc_hept_noscore,
labels = colnames(pc_hept_noscore)
)
plot(pc_iris$scores)
plot(pc_hept_noscore$scores)
names(pc_hept_noscore)
source('~/Desktop/lectures/[07] The Curse of Dimensionality Lecture Code.R')
install.packages(glmnet)
install.packages("glmnet")
source('~/Desktop/lectures/[07] The Curse of Dimensionality Lecture Code.R')
plot(pc_iris$scores) #Projected data: 2 dimensions.
plot(pc_iris$scores) #Projected data: 2 dimensions.
plot(iris_meas) #Original data: 4 dimensions.
plot(pc_hept_noscore$r.scores)
plot(pc_hept_noscore$scores)
factor.plot(pc_iris,
labels = colnames(iris_meas)) #Add variable names to the plot.
plot(pc_iris$scores) #Projected data: 2 dimensions.
class(pc_hept_noscore)
class(pc_iris$scores)
class(pc_iris)
class(pc_hept_noscore$scores)
plot(pc_iris$r.scores) #Projected data: 2 dimensions.
names(pc_iris)
class(iris_meas)
fa.parallel(hept_noscore_df,
n.obs = 25,
fa = "pc",
n.iter = 100)
hept_noscore_df = select(hept_df, -score)
hept_noscore_mat = as.matrix(hept_noscore_df)
hept_noscore_cor = cor(hept_noscore_mat)
fa.parallel(hept_noscore_df,
n.obs = 25,
fa = "pc",
n.iter = 100)
abline(h = 1) #Adding a horizontal line at 1.
pc_hept_noscore = principal(hept_noscore_df,
nfactors = 2,
rotate = "none")
pc_hept_noscore
plot(pc_hept_noscore$scores)
plot(pc_hept_noscore$scores,
labels = rownames(pc_hept_noscore)
)
plot(pc_hept_noscore$scores,
labels = rows(pc_hept_noscore)
)
plot(pc_hept_noscore$scores,
labels = row(pc_hept_noscore)
)
plot(pc_hept_noscore$scores)
plot(pc_hept_noscore$scores)
source('~/Desktop/homework/CurseDimensionality/adamcone.R')
View(hept_df)
prostate_df = read.csv("/Users/adamcone/Desktop/homework/CurseDimensionality/
[07] Prostate.txt")
prostate_df = read.csv("/Users/adamcone/Desktop/homework/CurseDimensionality/[07] Prostate.txt", sep = " ")
prostate_df = read.csv("/Users/adamcone/Desktop/homework/CurseDimensionality/[07] Prostate.txt", sep = " ")
prostate_df = read.csv("/Users/adamcone/Desktop/homework/CurseDimensionality/
[07] Prostate.txt", sep = " ")
prostate_df = read.csv("/Users/adamcone/Desktop/homework/CurseDimensionality/
[07] Prostate.txt", sep = " "
)
prostate_df = read.csv("/Users/adamcone/Desktop/homework/CurseDimensionality/[07] Prostate.txt", sep = " "
)
source('~/Desktop/homework/CurseDimensionality/adamcone.R')
pc_hept_noscore
as.date
?as.date
library(chron)
?as.date
source('~/Desktop/lectures/[07] The Curse of Dimensionality Lecture Code.R')
model.matrix(Salary ~ ., Hitters)
head(model.matrix(Salary ~ ., Hitters))
help(Hitters)
source('~/Desktop/lectures/[07] The Curse of Dimensionality Lecture Code.R', echo=TRUE)
grid = 10^seq(5, -2, length = 100)
grid
?"glmnet"
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)
ride.models
ridge.models
?as.Date
?sampl
?sample
source('~/Desktop/homework/CurseDimensionality/adamcone.R', echo=TRUE)
head(prostate_df)
# Question 1: Principal Component Analysis
library("psych")
library("dplyr")
library("HSAUR")
data(heptathlon)
# 1.1
# plot(heptathlon)
# intentionally commented, so I don't see it every I run my code.
# There are 56 plots on the 8 variables, and 25 observations in the data set.
# There are some pairwise relationships between variables that appear roughly
# linear, like run200 vs. score and longjump vs. hurdles. The highjump plots
# appear to have a significant outlier. Some, like javelin vs. shot, appear
# to have little recognizable correlations.
# 1.2
hept_df = mutate(heptathlon,
hurdles_t = max(hurdles) - hurdles,
run200m_t = max(run200m) - run200m,
run800m_t = max(run800m) - run800m
) %>%
select(., - hurdles, -run200m, -run800m)
# 1.3
# plot(heptathlon)
# intentionally commented, so I don't see it every I run my code.
# I don't see a difference here, and I wouldn't expect to. The transformations
# I performed above are translations only: I don't see how they would effect
# the relationships of any of these variables.
# 1.4
# first, I'll create the covariance matrix:
hept_noscore_df = select(hept_df, -score)
fa.parallel(hept_noscore_df,
n.obs = 25,
fa = "pc",
n.iter = 100)
abline(h = 1) #Adding a horizontal line at 1.
# Method 1: Take every principal component above the "elbow". In this case,
#           I see the elbow at principal component 3, so I would extract PCs 1
#           and 2.
# Method 2: Take every PC with eigenvalue greater than 1. In this case, I would
#           also take PCs 1 and 2.
# Method 3: Take every PC above the PC Simulated Data reference line. In this
#           case, I would take PC 1 only.
# I will take PCs 1 and 2, since I interpret 2 of 3 of the above methods to
# advocate this. Majority rules.
# 1.5
pc_hept_noscore = principal(hept_noscore_df,
nfactors = 2,
rotate = "none")
# 1.6
# lambda_1 = 4.46
# lambda_2 = 1.19
# 1.7
# proportion of variance captured by PC_1 = 0.64
# proportion of variance captured by PC_2 = 0.17
# 1.8
factor.plot(pc_hept_noscore,
labels = colnames(pc_hept_noscore)
)
# 1.9
# PC_1: The first principal component is strongly correlated with six of the
#       original 7 variables. The first principal component increases with
#       increasing highjump, shot, longjump, hurdles_t, run200m_t, and run800_t
#       scores. This suggests that these six criteria vary together. If one
#       increases, then the remaining five also increase.
#       Furthermore, I see that the first principal component correlates most
#       strongly with longjump and hurdles_t (loading = 0.96).
#       Basically, if an observation has a high value for any of these six
#       original variables, that observation is likey to have a high value
#       for the other five variables, and vice-versa.
# PC_2: The second principal component increases with only one of the original
#       variables: javelin. It appears to me that javelin scores are unrelated
#       to the other six scores, but still account for a great deal of overall
#       variability in the data.
# 1.10
plot(pc_hept_noscore$scores)
# 1.11
# It appears to me that there is an obvious outlier with low PC1 and high PC2.
# This indicates a high javelin score and a low score on everything else. I
# guess this is observation 25. Observation 25 appears to be an outlier on this
# plot because no other observations are close to it, and all the other
# observations are clustered relatively tightly. It could be that removing this
# one observation from the data would significantly change the PCA. I speculate
# that PC2 had a high variance specifically because of this outlier. Without
# this outlier, the remaining data may have been safely expressed by a single
# principal component. Speculation, of course.
source('~/Desktop/homework/CurseDimensionality/adamcone.R', echo=TRUE)
View(prostate_df)
?nrows
nrow
?nrow
nrow(prostate_df)
?sample
head(prostate_df)
prostate_rn = mutate(prostate_df, Row_Number = 1:obs)
obs = nrow(prostate_df)
prostate_rn = mutate(prostate_df, Row_Number = 1:obs)
View(prostate_rn)
training_df = filter(prostate_rn, Row_Number == training_indices)
obs = nrow(prostate_df)
n_train = round(0.8*obs)
n_test = obs-n_train
training_indices = sample(1:97, n_train, replace = FALSE)
prostate_rn = mutate(prostate_df, Row_Number = 1:obs)
training_df = filter(prostate_rn, Row_Number == training_indices)
glimpse(prostate_rn)
training_df = filter(prostate_rn, Row_Number %in% training_indices)
training_df = filter(prostate_rn, Row_Number %in% training_indices)
View(training_df)
round(0.8*obs)
training_indices
test_df = filter(prostate_rn, Row_Number !%in% training_indices) %>%
select(., -Row_Number)
test_df = filter(prostate_rn, !(Row_Number %in% training_indices)) %>%
select(., -Row_Number)
View(test_df)
19+78
source('~/Desktop/homework/CurseDimensionality/adamcone.R', echo=TRUE)
candidate_lambdas = 10^seq(5, -2, length = 100)
candidate_lambdas = 10^seq(5, -2, length = 100)
library(glmnet)
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)
View(training_df)
x = model.matrix(lpsa ~ ., training_df)[, -1]
y = Hitters$lpsa
candidate_lambdas = 10^seq(5, -2, length = 100)
library(glmnet)
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)
x = model.matrix(lpsa ~ ., training_df)[, -1]
y = training_df$lpsa
candidate_lambdas = 10^seq(5, -2, length = 100)
library(glmnet)
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)
x = model.matrix(lpsa ~ ., training_df)[, -1]
y = training_df$lpsa
candidate_lambdas = 10^seq(5, -2, length = 100)
library(glmnet)
ridge.models = glmnet(x,
y,
alpha = 0,
lambda = candidate_lambdas
)
ridge.models
dim(coef(ridge.models)) #20 different coefficients, estimated 100 times --
#once each per lambda value.
coef(ridge.models)
#Visualizing the ridge regression shrinkage.
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
?abline
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
abline(h = 0)
x = model.matrix(lpsa ~ ., training_df)[, -1]
y = training_df$lpsa
set.seed(0)
train = sample(1:nrow(x), 8*nrow(x)/10)
test = (-train)
y.test = y[test]
# 2.2
candidate_lambdas = 10^seq(5, -2, length = 100)
library(glmnet)
ridge.models = glmnet(x[train,],
y[train],
alpha = 0,
lambda = candidate_lambdas
)
dim(coef(ridge.models))
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
abline(h = 0)
set.seed(0)
cv.ridge.out = cv.glmnet(x[train, ], y[train], alpha = 0, nfolds = 10)
plot(cv.ridge.out, main = "Ridge Regression\n")
bestlambda.ridge = cv.ridge.out$lambda.min
bestlambda.ridge
log(bestlambda.ridge)
cv.ridge.out = cv.glmnet(x[train, ], y[train], alpha = 0, nfolds = 10)
plot(cv.ridge.out, main = "Ridge Regression\n")
source('~/Desktop/homework/CurseDimensionality/adamcone.R', echo=TRUE)
source('~/Desktop/homework/CurseDimensionality/adamcone.R', echo=TRUE)
plot(cv.ridge.out, main = "Ridge Regression\n")
source('~/Desktop/homework/CurseDimensionality/adamcone.R', echo=TRUE)
source('~/Desktop/homework/CurseDimensionality/adamcone.R', echo=TRUE)
library(ISLR)
Hitters = na.omit(Hitters)
help(Hitters)
class(Hitters)
source('~/Desktop/homework/CurseDimensionality/adamcone.R', echo=TRUE)
source('~/Desktop/homework/CurseDimensionality/adamcone.R', echo=TRUE)
bestlambda.ridge
log(bestlambda.ridge)
exp(0.1352036)
log(10)
log(-2.000974)
exp(-2.000974)
candidate_lambdas = 10^seq(7, -4, length = 500)
library(glmnet)
ridge.models = glmnet(x[train,],
y[train],
alpha = 0,
lambda = candidate_lambdas
)
dim(coef(ridge.models))
# 9 different coefficients, estimated 100 times
# 2.3
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
abline(h = 0)
# From the plot, I observe that: (1) variables 1, 2, and 5 appear to be the
# most significant for lower lambdas; (2) for lambdas above about log lambda
# = 3, there isn't much distinction between the lambdas; (3) above about log
# lambda = 5, all coefficients are essentially 0; (4) for lower lambdas,
# variables 3 and 6 have negative coefficients.
# 2.4
set.seed(0)
cv.ridge.out = cv.glmnet(x[train, ], y[train], alpha = 0, nfolds = 10)
# 2.5
plot(cv.ridge.out, main = "Ridge Regression\n")
# It appears to me that the
# 2.6
bestlambda.ridge = cv.ridge.out$lambda.min
bestlambda.ridge
log(bestlambda.ridge)
# 2.7
# 2.8
# 2.9
# Question 3: Lasso Regression
# 3.1
# 3.2
source('~/Desktop/homework/CurseDimensionality/adamcone.R', echo=TRUE)
set.seed(0)
cv.ridge.out = cv.glmnet(x[train, ], y[train], alpha = 0, nfolds = 10)
# 2.5
plot(cv.ridge.out, main = "Ridge Regression\n")
# It appears to me that the
# 2.6
bestlambda.ridge = cv.ridge.out$lambda.min
bestlambda.ridge
log(bestlambda.ridge)
bestlambda.ridge
ridge.bestlambdatrain = predict(ridge.models,
s = bestlambda.ridge,
newx = x[test, ])
mean((ridge.bestlambdatrain - y.test)^2)
View(prostate_df)
ridge.out = glmnet(x, y, alpha = 0)
predict(ridge.out, type = "coefficients", s = bestlambda.ridge)
predict(cv.ridge.out, type = "coefficients", s = bestlambda.ridge)
predict(ridge.out, type = "coefficients", s = bestlambda.ridge)
predict(cv.ridge.out, type = "coefficients", s = bestlambda.ridge)
ridge.out = glmnet(x, y, alpha = 0)
predict(ridge.out, type = "coefficients", s = bestlambda.ridge)
a = predict(cv.ridge.out, type = "coefficients", s = bestlambda.ridge)
ridge.out = glmnet(x, y, alpha = 0)
b = predict(ridge.out, type = "coefficients", s = bestlambda.ridge)
abs(abs(a)-abs(b))
all(abs(abs(a)-abs(b)) > 0)
ridge.bestlambda = predict(ridge.out, s = bestlambda.ridge, newx = x)
mean((ridge.bestlambda - y)^2)
x = model.matrix(lpsa ~ ., prostate_df)[, -1]
y = prostate_df$lpsa
set.seed(0)
train = sample(1:nrow(x), 8*nrow(x)/10)
test = (-train)
y.test = y[test]
# 2.2
candidate_lambdas = 10^seq(7, -4, length = 500)
library(glmnet)
lasso.models = glmnet(x[train,],
y[train],
alpha = 1,
lambda = candidate_lambdas
)
dim(coef(lasso.models))
plot(lasso.models, xvar = "lambda", label = TRUE, main = "Lasso Regression")
abline(h = 0)
?cv.glmnet
cv.lasso.out = cv.glmnet(x[train, ], y[train], alpha = 1, nfolds = 10)
plot(cv.lasso.out, main = "Ridge Regression\n")
plot(cv.ridge.out, main = "Ridge Regression\n")
plot(cv.lasso.out, main = "Ridge Regression\n")
bestlambda.lasso = cv.lasso.out$lambda.min
bestlambda.lasso
lasso.bestlambdatrain = predict(lasso.models,
s = bestlambda.lasso,
newx = x[test, ])
mean((lasso.bestlambdatrain - y.test)^2)
a = predict(cv.lasso.out, type = "coefficients", s = bestlambda.lasso)
lasso.out = glmnet(x, y, alpha = 1)
b = predict(lasso.out, type = "coefficients", s = bestlambda.lasso)
all(abs(abs(a)-abs(b)) > 0)
lasso.bestlambda = predict(lasso.out, s = bestlambda.lasso, newx = x)
mean((lasso.bestlambda - y)^2)
bestlambda.lasso
library("devtools", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("rsconnect", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("shinyapps", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
shiny::runApp('Desktop/Shiny Tutorial')
library("devtools", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("rsconnect", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("rstudioapi", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
detach("package:rstudioapi", unload=TRUE)
library("shinyapps", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("shinydashboard", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
shiny::runApp('Desktop/Shiny Tutorial')
rsconnect::setAccountInfo(name='adamcone',
token='2B8862742A78CEEEA2211217DF3DC162',
secret='hoVPAe3w59MKABwVU7ikPJjNCjIRHzOy63S3iGKs')
rsconnect::setAccountInfo(name='adamcone',
token='2B8862742A78CEEEA2211217DF3DC162',
secret='hoVPAe3w59MKABwVU7ikPJjNCjIRHzOy63S3iGKs')
shiny::runApp('Desktop/Shiny Tutorial')
library(rsconnect)
rsconnect::deployApp('path/to/your/app')
library(rsconnect)
rsconnect::deployApp('/Users/adamcone/Desktop/Shiny Tutorial')
rsconnect::setAccountInfo(name='adamcone',
token='2B8862742A78CEEEA2211217DF3DC162',
secret='hoVPAe3w59MKABwVU7ikPJjNCjIRHzOy63S3iGKs')
install.packages('rsconnect')
library(rsconnect)
rsconnect::deployApp('/Users/adamcone/Desktop/projects/Shiny Project/App')
shiny::runApp('Desktop/projects/Shiny Project/App')
load("./shinydataframe.RData")
load("~/Desktop/projects/Shiny Project/App/shinydataframe.RData")
View(items_filter)
View(items_tbl)
library("shiny")
library("dplyr")
library("ggplot2")
# I will create a function that inputs the parameters the user is prompted for,
# and main data frame items_tbl, and outputs a data frame that will be used for
# plotting the histogram.
items_filter = function(items_tbl,
Price_Range_selected,
Payment_Types_selected,
Days_of_Week_selected,
Times_of_Day_selected) {
stopifnot(class(Price_Range_selected) == "integer",
class(Payment_Types_selected) == "character",
class(Days_of_Week_selected) == "character",
class(Times_of_Day_selected) == "character",
length(Price_Range_selected) == 2,
length(Payment_Types_selected) %in% 1:5,
length(Days_of_Week_selected) %in% 1:7,
length(Times_of_Day_selected) %in% 1:3
)
plot_tbl = filter(items_tbl, Unit_Cost >= Price_Range_selected[1] &
Unit_Cost <= Price_Range_selected[2] &
Payment_Type %in% Payment_Types_selected &
Day_of_Week %in% Days_of_Week_selected &
Time_Bin %in% Times_of_Day_selected
) %>%
select(., Item_Total, Date_Bin) %>%
group_by(., Date_Bin) %>%
summarise(., Revenue_Total = sum(Item_Total))
print(plot_tbl)
return(plot_tbl)
}
shiny::runApp('Desktop/projects/Shiny Project/App')
shiny::runApp('Desktop/projects/Shiny Project/App')
shiny::runApp('Desktop/projects/Shiny Project/App')
library(rsconnect)
deployApp()
getwd()
)
getwd()
setwd("/Users/adamcone/Desktop/projects/Shiny Project/App")
library(rsconnect)
deployApp()
# Question 1: Principal Component Analysis
?fa.parallel
n.iter = 100)
?fa.parallel
setwd("/Users/adamcone/Desktop/projects/Shiny Project/OpenProduceShiny")
library(rsconnect)
deployApp()
